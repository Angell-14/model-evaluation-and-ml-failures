{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/angelchaudhary/scikit-learn-pipelines-vs-custom-ml-workflows?scriptVersionId=292223160\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# End-to-End Machine Learning Pipeline: Scikit-learn Pipelines vs Custom ML Workflows","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Introduction\nBuilding an end-to-end machine learning system involves more than model training—it includes data preprocessing, feature engineering, validation, and ensuring consistency across the entire workflow. These steps are commonly implemented either using Scikit-learn’s built in Pipelines or through custom, manually structured ML pipelines. Choosing the right approach impacts reliability, scalability, and the risk of data leakage.\n\nThis case study aims to compare Scikit-learn Pipelines with custom ML workflows to understand their real-world trade-offs. While Scikit-learn enforces structure and safety, custom pipelines offer flexibility and control. Understanding both approaches is essential for building production-ready ML systems and for demonstrating strong system design thinking in interviews.\n\n## Approach\nWe solve the same machine learning problem using two approaches:\n1. An end-to-end Scikit-learn Pipeline using `Pipeline` and `ColumnTransformer`\n2. A custom-built ML workflow with explicit preprocessing and modeling steps\n\nBoth approaches are evaluated on performance, maintainability, and robustness to highlight their strengths and limitations.","metadata":{}},{"cell_type":"markdown","source":"# LET'S DO IT!!!!\n![funny gif](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExOTg0c3M5emUzNjdyMHo4ejJmZ20xY3ljcm1rMzZqMDJiem1vd2hkcCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/maNB0qAiRVAty/giphy.gif)","metadata":{}},{"cell_type":"markdown","source":"## Dataset Overview \n**This dataset is taken from kaggle competition, \"Student Score Prediction\"**. We'll use it to predict exam scores based on study habits, sleep patterns, attendance, and demographic information. This dataset contains both numerical and categorical features, making it suitable for demonstrating end-to-end ML pipeline design.","metadata":{}},{"cell_type":"markdown","source":"### Target Variable\n- `exam_score` (Regression)\n\n### Features\n- Numerical: `study_hours`, `sleep_hours`, `attendance_rate`\n- Categorical: `gender`, `course`, `study_method`\n- Ordinal: `sleep_quality`, `facility_rating`","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:36:33.777381Z","iopub.execute_input":"2026-01-16T13:36:33.778121Z","iopub.status.idle":"2026-01-16T13:36:35.694123Z","shell.execute_reply.started":"2026-01-16T13:36:33.778091Z","shell.execute_reply":"2026-01-16T13:36:35.693347Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(\"/kaggle/input/kaggle-dataset/train.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:36:35.695374Z","iopub.execute_input":"2026-01-16T13:36:35.695791Z","iopub.status.idle":"2026-01-16T13:36:36.909287Z","shell.execute_reply.started":"2026-01-16T13:36:35.69577Z","shell.execute_reply":"2026-01-16T13:36:36.908674Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id  age  gender   course  study_hours  class_attendance internet_access  \\\n0   0   21  female     b.sc         7.91              98.8              no   \n1   1   18   other  diploma         4.95              94.8             yes   \n2   2   20  female     b.sc         4.68              92.6             yes   \n3   3   19    male     b.sc         2.00              49.5             yes   \n4   4   23    male      bca         7.65              86.9             yes   \n\n   sleep_hours sleep_quality   study_method facility_rating exam_difficulty  \\\n0          4.9       average  online videos             low            easy   \n1          4.7          poor     self-study          medium        moderate   \n2          5.8          poor       coaching            high        moderate   \n3          8.3       average    group study            high        moderate   \n4          9.6          good     self-study            high            easy   \n\n   exam_score  \n0        78.3  \n1        46.7  \n2        99.0  \n3        63.9  \n4       100.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>gender</th>\n      <th>course</th>\n      <th>study_hours</th>\n      <th>class_attendance</th>\n      <th>internet_access</th>\n      <th>sleep_hours</th>\n      <th>sleep_quality</th>\n      <th>study_method</th>\n      <th>facility_rating</th>\n      <th>exam_difficulty</th>\n      <th>exam_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>21</td>\n      <td>female</td>\n      <td>b.sc</td>\n      <td>7.91</td>\n      <td>98.8</td>\n      <td>no</td>\n      <td>4.9</td>\n      <td>average</td>\n      <td>online videos</td>\n      <td>low</td>\n      <td>easy</td>\n      <td>78.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>18</td>\n      <td>other</td>\n      <td>diploma</td>\n      <td>4.95</td>\n      <td>94.8</td>\n      <td>yes</td>\n      <td>4.7</td>\n      <td>poor</td>\n      <td>self-study</td>\n      <td>medium</td>\n      <td>moderate</td>\n      <td>46.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>20</td>\n      <td>female</td>\n      <td>b.sc</td>\n      <td>4.68</td>\n      <td>92.6</td>\n      <td>yes</td>\n      <td>5.8</td>\n      <td>poor</td>\n      <td>coaching</td>\n      <td>high</td>\n      <td>moderate</td>\n      <td>99.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>19</td>\n      <td>male</td>\n      <td>b.sc</td>\n      <td>2.00</td>\n      <td>49.5</td>\n      <td>yes</td>\n      <td>8.3</td>\n      <td>average</td>\n      <td>group study</td>\n      <td>high</td>\n      <td>moderate</td>\n      <td>63.9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>23</td>\n      <td>male</td>\n      <td>bca</td>\n      <td>7.65</td>\n      <td>86.9</td>\n      <td>yes</td>\n      <td>9.6</td>\n      <td>good</td>\n      <td>self-study</td>\n      <td>high</td>\n      <td>easy</td>\n      <td>100.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:36:57.376599Z","iopub.execute_input":"2026-01-16T13:36:57.377197Z","iopub.status.idle":"2026-01-16T13:36:57.382236Z","shell.execute_reply.started":"2026-01-16T13:36:57.377171Z","shell.execute_reply":"2026-01-16T13:36:57.381554Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(630000, 13)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:37:06.106668Z","iopub.execute_input":"2026-01-16T13:37:06.107176Z","iopub.status.idle":"2026-01-16T13:37:06.113112Z","shell.execute_reply.started":"2026-01-16T13:37:06.107152Z","shell.execute_reply":"2026-01-16T13:37:06.112326Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"id                    int64\nage                   int64\ngender               object\ncourse               object\nstudy_hours         float64\nclass_attendance    float64\ninternet_access      object\nsleep_hours         float64\nsleep_quality        object\nstudy_method         object\nfacility_rating      object\nexam_difficulty      object\nexam_score          float64\ndtype: object"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:37:16.940223Z","iopub.execute_input":"2026-01-16T13:37:16.940561Z","iopub.status.idle":"2026-01-16T13:37:17.143143Z","shell.execute_reply.started":"2026-01-16T13:37:16.940509Z","shell.execute_reply":"2026-01-16T13:37:17.142554Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"id                  0\nage                 0\ngender              0\ncourse              0\nstudy_hours         0\nclass_attendance    0\ninternet_access     0\nsleep_hours         0\nsleep_quality       0\nstudy_method        0\nfacility_rating     0\nexam_difficulty     0\nexam_score          0\ndtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df[\"exam_score\"].describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:37:27.120014Z","iopub.execute_input":"2026-01-16T13:37:27.120601Z","iopub.status.idle":"2026-01-16T13:37:27.149946Z","shell.execute_reply.started":"2026-01-16T13:37:27.120576Z","shell.execute_reply":"2026-01-16T13:37:27.149307Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"count    630000.000000\nmean         62.506672\nstd          18.916884\nmin          19.599000\n25%          48.800000\n50%          62.600000\n75%          76.300000\nmax         100.000000\nName: exam_score, dtype: float64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"X = df.drop(columns=[\"exam_score\"])\ny = df[\"exam_score\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(\"Train shape:\", X_train.shape)\nprint(\"Test shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:38:22.334665Z","iopub.execute_input":"2026-01-16T13:38:22.335567Z","iopub.status.idle":"2026-01-16T13:38:22.544268Z","shell.execute_reply.started":"2026-01-16T13:38:22.335513Z","shell.execute_reply":"2026-01-16T13:38:22.543579Z"}},"outputs":[{"name":"stdout","text":"Train shape: (504000, 12)\nTest shape: (126000, 12)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Feature Categorization\nWe explicitly separate numerical and categorical features to ensure consistent preprocessing.","metadata":{}},{"cell_type":"code","source":"numerical_features = [\n    \"study_hours\",\n    \"sleep_hours\",\n    \"attendance_rate\"\n]\n\ncategorical_features = [\n    \"gender\",\n    \"course\",\n    \"study_method\",\n    \"sleep_quality\",\n    \"facility_rating\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:39:41.426148Z","iopub.execute_input":"2026-01-16T13:39:41.426869Z","iopub.status.idle":"2026-01-16T13:39:41.430343Z","shell.execute_reply.started":"2026-01-16T13:39:41.426844Z","shell.execute_reply":"2026-01-16T13:39:41.429694Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Baseline Model (No Pipeline)\nBefore building pipelines, we train a simple baseline model without structured preprocessing. This helps establish a reference point.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import root_mean_squared_error, r2_score\n\nbaseline_model = LinearRegression()\nbaseline_model.fit(pd.get_dummies(X_train, drop_first=True),y_train)\n# Predictions\nbaseline_preds = baseline_model.predict(pd.get_dummies(X_test, drop_first=True))\n\n# Evaluation\nrmse = root_mean_squared_error(y_test, baseline_preds)\nr2 = r2_score(y_test, baseline_preds)\n\nprint(f\"Baseline RMSE: {rmse:.2f}\")\nprint(f\"Baseline R² Score: {r2:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:42:05.70992Z","iopub.execute_input":"2026-01-16T13:42:05.710222Z","iopub.status.idle":"2026-01-16T13:42:06.484615Z","shell.execute_reply.started":"2026-01-16T13:42:05.710199Z","shell.execute_reply":"2026-01-16T13:42:06.484022Z"}},"outputs":[{"name":"stdout","text":"Baseline RMSE: 8.89\nBaseline R² Score: 0.778\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Observation (Baseline Model)\n\nThe baseline Linear Regression model achieves an RMSE of **8.89** and an R² score of **0.778**, indicating that the model is able to explain a significant portion of the variance in exam scores. However, this approach relies on manual one-hot encoding using `pd.get_dummies`, which introduces several limitations:\n- The preprocessing logic is **not reusable** for inference or deployment.\n- There is a **risk of feature mismatch** if the training and test sets contain different categorical levels.\n- Preprocessing is performed **outside the model**, making the workflow harder to maintain and scale.\n\nWhile this baseline provides a reasonable performance reference, it highlights the need for a more structured and reliable pipeline-based approach.","metadata":{}},{"cell_type":"markdown","source":"## Approach A - Scikit-learn Pipeline\nIn this approach, we use Scikit-learn’s `Pipeline` and `ColumnTransformer` to build a fully structured end-to-end ML workflow.\nThis ensures:\n- Consistent preprocessing across training and testing\n- No data leakage\n- Better readability and maintainability\n- Easier experimentation and deployment","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing with ColumnTransformer\n\nNumerical and categorical features require different preprocessing strategies.\nWe apply scaling to numerical features and one-hot encoding to categorical features\nusing a `ColumnTransformer`.","metadata":{}},{"cell_type":"code","source":"# Automatically infer feature types\nnumerical_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncategorical_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\nprint(\"Numerical features:\", numerical_features)\nprint(\"Categorical features:\", categorical_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:48:01.07685Z","iopub.execute_input":"2026-01-16T13:48:01.077161Z","iopub.status.idle":"2026-01-16T13:48:01.117236Z","shell.execute_reply.started":"2026-01-16T13:48:01.077131Z","shell.execute_reply":"2026-01-16T13:48:01.116583Z"}},"outputs":[{"name":"stdout","text":"Numerical features: ['id', 'age', 'study_hours', 'class_attendance', 'sleep_hours']\nCategorical features: ['gender', 'course', 'internet_access', 'sleep_quality', 'study_method', 'facility_rating', 'exam_difficulty']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))])\n\npreprocessor = ColumnTransformer(\n    transformers=[(\"num\", numeric_transformer, numerical_features),(\"cat\", categorical_transformer, categorical_features)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:48:33.097767Z","iopub.execute_input":"2026-01-16T13:48:33.09835Z","iopub.status.idle":"2026-01-16T13:48:33.102358Z","shell.execute_reply.started":"2026-01-16T13:48:33.098323Z","shell.execute_reply":"2026-01-16T13:48:33.101576Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\npipeline_lr = Pipeline(steps=[(\"preprocessing\", preprocessor),(\"model\", LinearRegression())])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:45:45.12352Z","iopub.execute_input":"2026-01-16T13:45:45.124404Z","iopub.status.idle":"2026-01-16T13:45:45.128685Z","shell.execute_reply.started":"2026-01-16T13:45:45.124366Z","shell.execute_reply":"2026-01-16T13:45:45.127851Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Cross-Validation Evaluation\n\nWe'll evaluate the pipeline using 5-fold cross-validation. RMSE is used as the primary metric for consistency with the baseline comparison.","metadata":{}},{"cell_type":"code","source":"pipeline_lr = Pipeline(steps=[\n    (\"preprocessing\", preprocessor),\n    (\"model\", LinearRegression())\n])\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\ncv_rmse = -cross_val_score(\n    pipeline_lr,\n    X_train,\n    y_train,\n    cv=kf,\n    scoring=\"neg_root_mean_squared_error\"\n)\n\nprint(f\"CV RMSE Mean: {cv_rmse.mean():.2f}\")\nprint(f\"CV RMSE Std: {cv_rmse.std():.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:48:48.086416Z","iopub.execute_input":"2026-01-16T13:48:48.087008Z","iopub.status.idle":"2026-01-16T13:48:55.711894Z","shell.execute_reply.started":"2026-01-16T13:48:48.086982Z","shell.execute_reply":"2026-01-16T13:48:55.71104Z"}},"outputs":[{"name":"stdout","text":"CV RMSE Mean: 8.90\nCV RMSE Std: 0.00\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"pipeline_lr.fit(X_train, y_train)\n\npipeline_preds = pipeline_lr.predict(X_test)\n\npipeline_rmse = root_mean_squared_error(y_test, pipeline_preds)\npipeline_r2 = r2_score(y_test, pipeline_preds)\n\nprint(f\"Pipeline RMSE: {pipeline_rmse:.2f}\")\nprint(f\"Pipeline R² Score: {pipeline_r2:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T13:49:34.145972Z","iopub.execute_input":"2026-01-16T13:49:34.146278Z","iopub.status.idle":"2026-01-16T13:49:35.982395Z","shell.execute_reply.started":"2026-01-16T13:49:34.146253Z","shell.execute_reply":"2026-01-16T13:49:35.981691Z"}},"outputs":[{"name":"stdout","text":"Pipeline RMSE: 8.89\nPipeline R² Score: 0.778\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Observation (Cross-Validation)\n\nThe pipeline achieves a mean cross-validation RMSE of **8.90** with a standard\ndeviation of **0.00**, indicating highly consistent performance across all folds.\n\nThis suggests that the model’s predictions are stable and not sensitive to different train-validation splits. Importantly, the performance closely matches the baseline model, confirming that the pipeline does not introduce data leakage or optimistic bias. The primary benefit of the pipeline lies not in improved accuracy, but in ensuring reliability, reproducibility, and consistency across the entire\nmachine learning workflow.","metadata":{}},{"cell_type":"markdown","source":"## Approach B: Custom ML Pipeline\n\nIn this approach, we manually implement each step of the machine learning workflow, including preprocessing, feature transformation, model training, and evaluation. While this method offers flexibility and transparency, it requires careful handling to avoid common pitfalls such as data leakage, feature mismatch, and inconsistent transformations between training and inference.","metadata":{}},{"cell_type":"markdown","source":"### Manual Feature Separation\n\nWe explicitly separate numerical and categorical features to apply custom preprocessing steps.","metadata":{}},{"cell_type":"code","source":"### Explicit feature separation\nnum_features = numerical_features\ncat_features = categorical_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T14:08:17.5488Z","iopub.execute_input":"2026-01-16T14:08:17.54935Z","iopub.status.idle":"2026-01-16T14:08:17.552885Z","shell.execute_reply.started":"2026-01-16T14:08:17.549325Z","shell.execute_reply":"2026-01-16T14:08:17.552072Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train_num = scaler.fit_transform(X_train[num_features])\nX_test_num = scaler.transform(X_test[num_features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T14:08:19.235025Z","iopub.execute_input":"2026-01-16T14:08:19.235274Z","iopub.status.idle":"2026-01-16T14:08:19.290325Z","shell.execute_reply.started":"2026-01-16T14:08:19.235256Z","shell.execute_reply":"2026-01-16T14:08:19.289808Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### Manual One-Hot Encoding of Categorical Features\n\nA `OneHotEncoder` is fitted on the training data and used to transform both training and test sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n\nX_train_cat = encoder.fit_transform(X_train[cat_features])\nX_test_cat = encoder.transform(X_test[cat_features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T14:08:51.281062Z","iopub.execute_input":"2026-01-16T14:08:51.281355Z","iopub.status.idle":"2026-01-16T14:08:52.361264Z","shell.execute_reply.started":"2026-01-16T14:08:51.281332Z","shell.execute_reply":"2026-01-16T14:08:52.360505Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"### Feature Concatenation\n\nNumerical and categorical features are manually combined to form the final feature matrices.","metadata":{}},{"cell_type":"code","source":"X_train_final = np.hstack([X_train_num, X_train_cat])\nX_test_final = np.hstack([X_test_num, X_test_cat])\n\nprint(\"Final train shape:\", X_train_final.shape)\nprint(\"Final test shape:\", X_test_final.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T14:09:17.353161Z","iopub.execute_input":"2026-01-16T14:09:17.353971Z","iopub.status.idle":"2026-01-16T14:09:17.42216Z","shell.execute_reply.started":"2026-01-16T14:09:17.353935Z","shell.execute_reply":"2026-01-16T14:09:17.421495Z"}},"outputs":[{"name":"stdout","text":"Final train shape: (504000, 31)\nFinal test shape: (126000, 31)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Model Training and Evaluation\n\nWe train the same Linear Regression model on the manually processed features and evaluate it on the test set.","metadata":{}},{"cell_type":"code","source":"manual_model = LinearRegression()\n\nmanual_model.fit(X_train_final, y_train)\n\nmanual_preds = manual_model.predict(X_test_final)\n\nmanual_rmse = root_mean_squared_error(y_test, manual_preds)\nmanual_r2 = r2_score(y_test, manual_preds)\n\nprint(f\"Manual Pipeline RMSE: {manual_rmse:.2f}\")\nprint(f\"Manual Pipeline R² Score: {manual_r2:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T14:09:44.465022Z","iopub.execute_input":"2026-01-16T14:09:44.465721Z","iopub.status.idle":"2026-01-16T14:09:45.050039Z","shell.execute_reply.started":"2026-01-16T14:09:44.465693Z","shell.execute_reply":"2026-01-16T14:09:45.049422Z"}},"outputs":[{"name":"stdout","text":"Manual Pipeline RMSE: 8.89\nManual Pipeline R² Score: 0.778\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Observation (Manual vs Pipeline Comparison)\n\nAll three approaches—baseline, Scikit-learn Pipeline, and custom manual pipeline produce nearly identical performance metrics. This confirms that the model learns the same underlying relationships when preprocessing is applied correctly. However, the key difference lies in *how safe and maintainable* each approach is. While the manual pipeline matches the pipeline’s performance in this controlled setting, it relies heavily on careful implementation and bookkeeping. As the workflow grows more complex, this approach becomes increasingly fragile.\n\nScikit-learn Pipelines provide the same performance guarantees while systematically preventing common errors such as data leakage, feature mismatch,and inconsistent inference logic.","metadata":{}},{"cell_type":"markdown","source":"## Final Comparison\n\n| Aspect | Baseline | Sklearn Pipeline | Manual Pipeline |\n|------|--------|----------------|----------------|\n| Performance | ✅ | ✅ | ✅ |\n| Data Leakage Risk | High | Low | Medium |\n| Reusability | Low | High | Medium |\n| Debuggability | Low | High | Medium |\n| Production Ready | ❌ | ✅ | ❌ |\n","metadata":{}},{"cell_type":"markdown","source":"## Key Takeaways\n\n- Model performance alone is not sufficient to judge an ML system.\n- Structured pipelines reduce human error without sacrificing accuracy.\n- Manual pipelines require strict discipline and do not scale well.\n- Scikit-learn Pipelines are the safest choice for production ML workflows.","metadata":{}}]}