{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/angelchaudhary/cross-validation-vs-single-split?scriptVersionId=292325757\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Cross-Validation vs Single Split: Performance Drift Analysis","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nIn many machine learning projects, models are evaluated using a single train–test split. While simple, this approach can produce unstable and misleading performance estimates depending on how the data is split. This case study investigates how model performance drifts when evaluated using a single split versus k-fold cross-validation.\n\nEvaluation strategy directly affects model trustworthiness. Two models trained on the same data can appear very different in performance purely due to data partition randomness. Understanding this drift is critical for building reliable, production-ready models, especially in data science competitions and real-world deployments.\n\n## Approach\n\nWe will:\n\n- Train the same model using a single train–test split and k-fold cross-validation\n\n- Compare performance metrics across folds and splits\n\n- Quantify performance variability and drift\n\n- Analyze when a single split is risky and when cross-validation provides more stable estimates\n\nThe goal is to build intuition around evaluation robustness, not just model accuracy.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# LET'S DO IT!!!\n![FUNNY GIF](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExNXhpNmdiODE5OWZoYzFzMDRjanR2Zmxja2J0YXNlZmFra3J6MGpseSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/13HBDT4QSTpveU/giphy.gif)","metadata":{}},{"cell_type":"markdown","source":"## Dataset Overview\n\nThis case study uses the Boston Housing dataset, a classic regression dataset that predicts the median value of owner-occupied homes (MEDV) based on socio-economic and environmental features. The dataset contains ~500 samples with 13 input features, including crime rate, number of rooms, property tax rate, and neighborhood characteristics. Its small size and inherent noise make model performance highly sensitive to how the data is split. These properties make the dataset especially suitable for analyzing performance drift between a single train–test split and k-fold cross-validation, highlighting the importance of robust evaluation strategies.","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"fedesoriano/the-boston-houseprice-data\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:39:25.069741Z","iopub.execute_input":"2026-01-17T06:39:25.070263Z","iopub.status.idle":"2026-01-17T06:39:25.268738Z","shell.execute_reply.started":"2026-01-17T06:39:25.070231Z","shell.execute_reply":"2026-01-17T06:39:25.266867Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/the-boston-houseprice-data\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\ndf = pd.read_csv(\"/kaggle/input/the-boston-houseprice-data/boston.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:40:24.459224Z","iopub.execute_input":"2026-01-17T06:40:24.46091Z","iopub.status.idle":"2026-01-17T06:40:24.983101Z","shell.execute_reply.started":"2026-01-17T06:40:24.460857Z","shell.execute_reply":"2026-01-17T06:40:24.981711Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:40:33.776917Z","iopub.execute_input":"2026-01-17T06:40:33.777378Z","iopub.status.idle":"2026-01-17T06:40:33.830507Z","shell.execute_reply.started":"2026-01-17T06:40:33.777334Z","shell.execute_reply":"2026-01-17T06:40:33.82937Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n\n   PTRATIO       B  LSTAT  MEDV  \n0     15.3  396.90   4.98  24.0  \n1     17.8  396.90   9.14  21.6  \n2     17.8  392.83   4.03  34.7  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:41:31.982756Z","iopub.execute_input":"2026-01-17T06:41:31.983187Z","iopub.status.idle":"2026-01-17T06:41:32.031524Z","shell.execute_reply.started":"2026-01-17T06:41:31.983149Z","shell.execute_reply":"2026-01-17T06:41:32.030502Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\ncount  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \nmean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \nstd      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \nmin      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \nmax     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n\n              AGE         DIS         RAD         TAX     PTRATIO           B  \\\ncount  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \nmean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \nstd     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \nmin      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \nmax    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n\n            LSTAT        MEDV  \ncount  506.000000  506.000000  \nmean    12.653063   22.532806  \nstd      7.141062    9.197104  \nmin      1.730000    5.000000  \n25%      6.950000   17.025000  \n50%     11.360000   21.200000  \n75%     16.955000   25.000000  \nmax     37.970000   50.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.613524</td>\n      <td>11.363636</td>\n      <td>11.136779</td>\n      <td>0.069170</td>\n      <td>0.554695</td>\n      <td>6.284634</td>\n      <td>68.574901</td>\n      <td>3.795043</td>\n      <td>9.549407</td>\n      <td>408.237154</td>\n      <td>18.455534</td>\n      <td>356.674032</td>\n      <td>12.653063</td>\n      <td>22.532806</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.601545</td>\n      <td>23.322453</td>\n      <td>6.860353</td>\n      <td>0.253994</td>\n      <td>0.115878</td>\n      <td>0.702617</td>\n      <td>28.148861</td>\n      <td>2.105710</td>\n      <td>8.707259</td>\n      <td>168.537116</td>\n      <td>2.164946</td>\n      <td>91.294864</td>\n      <td>7.141062</td>\n      <td>9.197104</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.006320</td>\n      <td>0.000000</td>\n      <td>0.460000</td>\n      <td>0.000000</td>\n      <td>0.385000</td>\n      <td>3.561000</td>\n      <td>2.900000</td>\n      <td>1.129600</td>\n      <td>1.000000</td>\n      <td>187.000000</td>\n      <td>12.600000</td>\n      <td>0.320000</td>\n      <td>1.730000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.082045</td>\n      <td>0.000000</td>\n      <td>5.190000</td>\n      <td>0.000000</td>\n      <td>0.449000</td>\n      <td>5.885500</td>\n      <td>45.025000</td>\n      <td>2.100175</td>\n      <td>4.000000</td>\n      <td>279.000000</td>\n      <td>17.400000</td>\n      <td>375.377500</td>\n      <td>6.950000</td>\n      <td>17.025000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.256510</td>\n      <td>0.000000</td>\n      <td>9.690000</td>\n      <td>0.000000</td>\n      <td>0.538000</td>\n      <td>6.208500</td>\n      <td>77.500000</td>\n      <td>3.207450</td>\n      <td>5.000000</td>\n      <td>330.000000</td>\n      <td>19.050000</td>\n      <td>391.440000</td>\n      <td>11.360000</td>\n      <td>21.200000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.677083</td>\n      <td>12.500000</td>\n      <td>18.100000</td>\n      <td>0.000000</td>\n      <td>0.624000</td>\n      <td>6.623500</td>\n      <td>94.075000</td>\n      <td>5.188425</td>\n      <td>24.000000</td>\n      <td>666.000000</td>\n      <td>20.200000</td>\n      <td>396.225000</td>\n      <td>16.955000</td>\n      <td>25.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>88.976200</td>\n      <td>100.000000</td>\n      <td>27.740000</td>\n      <td>1.000000</td>\n      <td>0.871000</td>\n      <td>8.780000</td>\n      <td>100.000000</td>\n      <td>12.126500</td>\n      <td>24.000000</td>\n      <td>711.000000</td>\n      <td>22.000000</td>\n      <td>396.900000</td>\n      <td>37.970000</td>\n      <td>50.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Feature–Target Split\n\nWe separate the dataset into input features (`X`) and the target variable (`y`). The target variable `MEDV` represents the median house value and will be used for regression modeling.","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=[\"MEDV\"])\ny = df[\"MEDV\"]\n\nprint(\"Feature shape:\", X.shape)\nprint(\"Target shape:\", y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:43:26.463998Z","iopub.execute_input":"2026-01-17T06:43:26.464337Z","iopub.status.idle":"2026-01-17T06:43:26.478532Z","shell.execute_reply.started":"2026-01-17T06:43:26.464311Z","shell.execute_reply":"2026-01-17T06:43:26.477261Z"}},"outputs":[{"name":"stdout","text":"Feature shape: (506, 13)\nTarget shape: (506,)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Baseline Evaluation: Single Train–Test Split\n\nWe first evaluate model performance using a single train–test split. This approach is simple but highly sensitive to how the data is split, which can lead to unstable performance estimates.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(n_estimators=200,random_state=42)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nrmse_single = np.sqrt(mean_squared_error(y_test, y_pred))\nr2_single = r2_score(y_test, y_pred)\n\nprint(f\"Single Split RMSE: {rmse_single:.3f}\")\nprint(f\"Single Split R²: {r2_single:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:44:12.790269Z","iopub.execute_input":"2026-01-17T06:44:12.79086Z","iopub.status.idle":"2026-01-17T06:44:13.814369Z","shell.execute_reply.started":"2026-01-17T06:44:12.790761Z","shell.execute_reply":"2026-01-17T06:44:13.813487Z"}},"outputs":[{"name":"stdout","text":"Single Split RMSE: 2.917\nSingle Split R²: 0.884\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Observation\n\nUsing a single 80–20 train–test split, the model achieves an RMSE of **2.92** and an R² of **0.88**, indicating strong predictive performance on this particular split.\n\nHowever, this evaluation reflects performance on **only one random partition of the data**. Given the relatively small size of the dataset, these metrics may be **overly optimistic or pessimistic** depending on how the data was split. As a result, this single-split score does not reliably capture the model’s true generalization ability. This motivates the need for **cross-validation** to assess performance stability and quantify potential evaluation drift.","metadata":{}},{"cell_type":"markdown","source":"## Robust Evaluation: K-Fold Cross-Validation\n\nTo obtain a more reliable estimate of model performance, we use k-fold cross-validation. This approach evaluates the model across multiple data splits and reduces variance caused by split randomness.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nrmse_scores = []\nr2_scores = []\n\nfor train_idx, val_idx in kf.split(X):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n    model = RandomForestRegressor(\n        n_estimators=200,\n        random_state=42\n    )\n    \n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n\n    rmse_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n    r2_scores.append(r2_score(y_val, preds))\n\nprint(\"CV RMSE Mean:\", np.mean(rmse_scores))\nprint(\"CV RMSE Std:\", np.std(rmse_scores))\nprint(\"CV R² Mean:\", np.mean(r2_scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T06:44:43.611321Z","iopub.execute_input":"2026-01-17T06:44:43.612869Z","iopub.status.idle":"2026-01-17T06:44:48.772163Z","shell.execute_reply.started":"2026-01-17T06:44:43.612776Z","shell.execute_reply":"2026-01-17T06:44:48.771206Z"}},"outputs":[{"name":"stdout","text":"CV RMSE Mean: 3.255151483931973\nCV RMSE Std: 0.4567376242917499\nCV R² Mean: 0.8705617147067851\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Observation\n\nUsing 5-fold cross-validation, the model achieves an average RMSE of **3.26** with a standard deviation of **0.46**, and a mean R² of **0.87**.\n\nCompared to the single train–test split, cross-validation reports slightly worse but more realistic performance. The non-trivial standard deviation in RMSE across folds indicates that model performance is sensitive to data partitioning, confirming the presence of evaluation drift.\n\nThis demonstrates that the single-split result (RMSE ≈ 2.92) was optimistic and cross validation provides a more reliable estimate of true generalization performance.","metadata":{}},{"cell_type":"markdown","source":"## Performance Drift: Single Split vs Cross Validation\n\nA clear performance drift is observed when comparing the single train–test split with cross-validation.\n\n- **Single Split RMSE:** ~2.92  \n- **Cross-Validation RMSE (Mean):** ~3.26  \n\nThe single split reports better performance, but this improvement is misleading as it is based on only one data partition. Cross-validation, by evaluating the model across multiple folds, exposes the variability in performance and provides a more conservative and stable estimate. The RMSE standard deviation across folds further highlights that model performance is not consistent across different subsets of data, reinforcing the risk of relying on a single split for evaluation.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion & Key Takeaways\n\nThis case study demonstrates that model performance can vary significantly depending on the evaluation strategy used.\n\nWhile a single train–test split reported strong performance, cross-validation revealed a **more realistic and stable estimate** of generalization by accounting for variability across multiple data splits. The observed performance drift highlights the risk of drawing conclusions from a single evaluation run, especially on small and noisy datasets.\n\nKey takeaways:\n- Single train–test splits can produce **optimistic or misleading results**\n- Cross-validation reduces evaluation bias and improves reliability\n- Performance stability is as important as raw accuracy when assessing models\n\nOverall, cross-validation should be preferred when model evaluation quality and robustness are critical.","metadata":{}}]}