{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":5715902,"datasetId":3286194,"databundleVersionId":5792029}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/angelchaudhary/scaling-techniques-comparison?scriptVersionId=292719320\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Scaling Techniques Compared: StandardScaler vs MinMaxScaler vs RobustScaler","metadata":{}},{"cell_type":"markdown","source":"## Introduction\n\nMachine learning algorithms are highly sensitive to the scale of input features. When features exist on different ranges or contain extreme values (outliers), models may converge slowly, assign incorrect importance to features, or produce suboptimal results. Choosing an inappropriate scaling technique can therefore negatively impact model performance and interpretability.\n\nAlthough feature scaling is a fundamental preprocessing step, it is often applied without understanding how different scalers behave under varying data conditions. StandardScaler, MinMaxScaler, and RobustScaler each make different assumptions about data distribution and outliers.\n\nThis case study aims to provide a **clear, hands-on comparison** of these three widely used scaling techniques, highlighting:\n- How each scaler transforms the data\n- Their sensitivity to outliers\n- Their impact on model performance\n\nIn this notebook, we follow a structured and experimental approach:\n1. Select a real-world dataset with features on different scales and the presence of outliers.\n2. Apply **StandardScaler**, **MinMaxScaler**, and **RobustScaler** independently to the same dataset.\n3. Visualize and analyze how each scaler transforms feature distributions.\n4. Train identical machine learning models on the scaled data.\n5. Compare model performance using consistent evaluation metrics.\n6. Draw practical conclusions and recommendations for real-world use cases.","metadata":{}},{"cell_type":"markdown","source":"# LET'S DO IT!!!!\n![FUNNY GIF](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZ2p1ejR6OGxlNXFyOGVmaThza2VubjZhazJoYTJjNzN6Mmk1ajF1NyZlcD12MV9naWZzX3NlYXJjaCZjdD1n/LHZyixOnHwDDy/giphy.gif)","metadata":{}},{"cell_type":"markdown","source":"## Dataset Overview\n\nWe'll use the **California Housing Prices Dataset** \nWhy this dataset?\n- All numerical features\n- Strong variation in feature scales\n- Presence of skewness and outliers\n- Real-world regression problem\n\nTarget Variable:\n- `median_house_value`","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"/kaggle/input/california-housing-prices-dataset/housing.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:08:26.193425Z","iopub.execute_input":"2026-01-19T11:08:26.19379Z","iopub.status.idle":"2026-01-19T11:08:26.668866Z","shell.execute_reply.started":"2026-01-19T11:08:26.193756Z","shell.execute_reply":"2026-01-19T11:08:26.668025Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                  41          880           129.0   \n1    -122.22     37.86                  21         7099          1106.0   \n2    -122.24     37.85                  52         1467           190.0   \n3    -122.25     37.85                  52         1274           235.0   \n4    -122.25     37.85                  52         1627           280.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0         322         126         8.3252              452600        NEAR BAY  \n1        2401        1138         8.3014              358500        NEAR BAY  \n2         496         177         7.2574              352100        NEAR BAY  \n3         558         219         5.6431              341300        NEAR BAY  \n4         565         259         3.8462              342200        NEAR BAY  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41</td>\n      <td>880</td>\n      <td>129.0</td>\n      <td>322</td>\n      <td>126</td>\n      <td>8.3252</td>\n      <td>452600</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21</td>\n      <td>7099</td>\n      <td>1106.0</td>\n      <td>2401</td>\n      <td>1138</td>\n      <td>8.3014</td>\n      <td>358500</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52</td>\n      <td>1467</td>\n      <td>190.0</td>\n      <td>496</td>\n      <td>177</td>\n      <td>7.2574</td>\n      <td>352100</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52</td>\n      <td>1274</td>\n      <td>235.0</td>\n      <td>558</td>\n      <td>219</td>\n      <td>5.6431</td>\n      <td>341300</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52</td>\n      <td>1627</td>\n      <td>280.0</td>\n      <td>565</td>\n      <td>259</td>\n      <td>3.8462</td>\n      <td>342200</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:08:38.456387Z","iopub.execute_input":"2026-01-19T11:08:38.456692Z","iopub.status.idle":"2026-01-19T11:08:38.481827Z","shell.execute_reply.started":"2026-01-19T11:08:38.456665Z","shell.execute_reply":"2026-01-19T11:08:38.480835Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  int64  \n 3   total_rooms         20640 non-null  int64  \n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  int64  \n 6   households          20640 non-null  int64  \n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  int64  \n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(4), int64(5), object(1)\nmemory usage: 1.6+ MB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:08:48.37679Z","iopub.execute_input":"2026-01-19T11:08:48.37711Z","iopub.status.idle":"2026-01-19T11:08:48.420493Z","shell.execute_reply.started":"2026-01-19T11:08:48.377072Z","shell.execute_reply":"2026-01-19T11:08:48.419208Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"          longitude      latitude  housing_median_age   total_rooms  \\\ncount  20640.000000  20640.000000        20640.000000  20640.000000   \nmean    -119.569704     35.631861           28.639486   2635.763081   \nstd        2.003532      2.135952           12.585558   2181.615252   \nmin     -124.350000     32.540000            1.000000      2.000000   \n25%     -121.800000     33.930000           18.000000   1447.750000   \n50%     -118.490000     34.260000           29.000000   2127.000000   \n75%     -118.010000     37.710000           37.000000   3148.000000   \nmax     -114.310000     41.950000           52.000000  39320.000000   \n\n       total_bedrooms    population    households  median_income  \\\ncount    20433.000000  20640.000000  20640.000000   20640.000000   \nmean       537.870553   1425.476744    499.539680       3.870671   \nstd        421.385070   1132.462122    382.329753       1.899822   \nmin          1.000000      3.000000      1.000000       0.499900   \n25%        296.000000    787.000000    280.000000       2.563400   \n50%        435.000000   1166.000000    409.000000       3.534800   \n75%        647.000000   1725.000000    605.000000       4.743250   \nmax       6445.000000  35682.000000   6082.000000      15.000100   \n\n       median_house_value  \ncount        20640.000000  \nmean        206855.816909  \nstd         115395.615874  \nmin          14999.000000  \n25%         119600.000000  \n50%         179700.000000  \n75%         264725.000000  \nmax         500001.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20433.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n      <td>20640.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-119.569704</td>\n      <td>35.631861</td>\n      <td>28.639486</td>\n      <td>2635.763081</td>\n      <td>537.870553</td>\n      <td>1425.476744</td>\n      <td>499.539680</td>\n      <td>3.870671</td>\n      <td>206855.816909</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.003532</td>\n      <td>2.135952</td>\n      <td>12.585558</td>\n      <td>2181.615252</td>\n      <td>421.385070</td>\n      <td>1132.462122</td>\n      <td>382.329753</td>\n      <td>1.899822</td>\n      <td>115395.615874</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-124.350000</td>\n      <td>32.540000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.499900</td>\n      <td>14999.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-121.800000</td>\n      <td>33.930000</td>\n      <td>18.000000</td>\n      <td>1447.750000</td>\n      <td>296.000000</td>\n      <td>787.000000</td>\n      <td>280.000000</td>\n      <td>2.563400</td>\n      <td>119600.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-118.490000</td>\n      <td>34.260000</td>\n      <td>29.000000</td>\n      <td>2127.000000</td>\n      <td>435.000000</td>\n      <td>1166.000000</td>\n      <td>409.000000</td>\n      <td>3.534800</td>\n      <td>179700.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-118.010000</td>\n      <td>37.710000</td>\n      <td>37.000000</td>\n      <td>3148.000000</td>\n      <td>647.000000</td>\n      <td>1725.000000</td>\n      <td>605.000000</td>\n      <td>4.743250</td>\n      <td>264725.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-114.310000</td>\n      <td>41.950000</td>\n      <td>52.000000</td>\n      <td>39320.000000</td>\n      <td>6445.000000</td>\n      <td>35682.000000</td>\n      <td>6082.000000</td>\n      <td>15.000100</td>\n      <td>500001.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df = df.drop(columns=[\"total_bedrooms\", \"ocean_proximity\"])\ndf.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:09:01.530415Z","iopub.execute_input":"2026-01-19T11:09:01.530799Z","iopub.status.idle":"2026-01-19T11:09:01.544778Z","shell.execute_reply.started":"2026-01-19T11:09:01.530768Z","shell.execute_reply":"2026-01-19T11:09:01.54382Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"longitude             0\nlatitude              0\nhousing_median_age    0\ntotal_rooms           0\npopulation            0\nhouseholds            0\nmedian_income         0\nmedian_house_value    0\ndtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"X = df.drop(columns=[\"median_house_value\"])\ny = df[\"median_house_value\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:09:31.496664Z","iopub.execute_input":"2026-01-19T11:09:31.496995Z","iopub.status.idle":"2026-01-19T11:09:31.503061Z","shell.execute_reply.started":"2026-01-19T11:09:31.496968Z","shell.execute_reply":"2026-01-19T11:09:31.502177Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:09:53.316145Z","iopub.execute_input":"2026-01-19T11:09:53.316492Z","iopub.status.idle":"2026-01-19T11:09:54.257189Z","shell.execute_reply.started":"2026-01-19T11:09:53.316461Z","shell.execute_reply":"2026-01-19T11:09:54.25618Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Scaling Techniques Applied\n\nEach scaler is fitted **only on training data** to avoid data leakage.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\nstandard_scaler = StandardScaler()\nminmax_scaler = MinMaxScaler()\nrobust_scaler = RobustScaler()\n\nX_train_standard = standard_scaler.fit_transform(X_train)\nX_train_minmax = minmax_scaler.fit_transform(X_train)\nX_train_robust = robust_scaler.fit_transform(X_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:10:17.167515Z","iopub.execute_input":"2026-01-19T11:10:17.168049Z","iopub.status.idle":"2026-01-19T11:10:17.196697Z","shell.execute_reply.started":"2026-01-19T11:10:17.168016Z","shell.execute_reply":"2026-01-19T11:10:17.195565Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"X_train_standard_df = pd.DataFrame(\n    X_train_standard, columns=X_train.columns, index=X_train.index\n)\n\nX_train_minmax_df = pd.DataFrame(\n    X_train_minmax, columns=X_train.columns, index=X_train.index\n)\n\nX_train_robust_df = pd.DataFrame(\n    X_train_robust, columns=X_train.columns, index=X_train.index\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:10:25.853347Z","iopub.execute_input":"2026-01-19T11:10:25.853685Z","iopub.status.idle":"2026-01-19T11:10:25.860085Z","shell.execute_reply.started":"2026-01-19T11:10:25.853657Z","shell.execute_reply":"2026-01-19T11:10:25.859074Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Distribution Comparison: Median Income Feature\n\nWe compare how each scaler transforms the same feature (`median_income`) to understand differences in representation and outlier handling.","metadata":{}},{"cell_type":"code","source":"scaling_comparison = pd.DataFrame({\n    \"Original\": X_train[\"median_income\"].describe(),\n    \"StandardScaler\": X_train_standard_df[\"median_income\"].describe(),\n    \"MinMaxScaler\": X_train_minmax_df[\"median_income\"].describe(),\n    \"RobustScaler\": X_train_robust_df[\"median_income\"].describe()\n})\n\nscaling_comparison","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:13:44.524907Z","iopub.execute_input":"2026-01-19T11:13:44.52541Z","iopub.status.idle":"2026-01-19T11:13:44.549062Z","shell.execute_reply.started":"2026-01-19T11:13:44.525376Z","shell.execute_reply":"2026-01-19T11:13:44.548195Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"           Original  StandardScaler  MinMaxScaler  RobustScaler\ncount  16512.000000    1.651200e+04  16512.000000  1.651200e+04\nmean       3.880754   -6.519333e-17      0.233159  1.518051e-01\nstd        1.904294    1.000030e+00      0.131329  8.630480e-01\nmin        0.499900   -1.775438e+00      0.000000 -1.380437e+00\n25%        2.566700   -6.900689e-01      0.142536 -4.437394e-01\n50%        3.545800   -1.758995e-01      0.210059  1.006411e-16\n75%        4.773175    4.686502e-01      0.294705  5.562606e-01\nmax       15.000100    5.839268e+00      1.000000  5.191221e+00","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original</th>\n      <th>StandardScaler</th>\n      <th>MinMaxScaler</th>\n      <th>RobustScaler</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>16512.000000</td>\n      <td>1.651200e+04</td>\n      <td>16512.000000</td>\n      <td>1.651200e+04</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.880754</td>\n      <td>-6.519333e-17</td>\n      <td>0.233159</td>\n      <td>1.518051e-01</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.904294</td>\n      <td>1.000030e+00</td>\n      <td>0.131329</td>\n      <td>8.630480e-01</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.499900</td>\n      <td>-1.775438e+00</td>\n      <td>0.000000</td>\n      <td>-1.380437e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.566700</td>\n      <td>-6.900689e-01</td>\n      <td>0.142536</td>\n      <td>-4.437394e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.545800</td>\n      <td>-1.758995e-01</td>\n      <td>0.210059</td>\n      <td>1.006411e-16</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.773175</td>\n      <td>4.686502e-01</td>\n      <td>0.294705</td>\n      <td>5.562606e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>15.000100</td>\n      <td>5.839268e+00</td>\n      <td>1.000000</td>\n      <td>5.191221e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Observations Based on Scaling Output (Median Income)\n\n- In the **original feature**, `median_income` spans a wide range (≈ 0.5 to 15), indicating strong variability and the presence of high-end values.\n\n- After applying **StandardScaler**:\n  - The feature is centered around **mean ≈ 0** with **standard deviation ≈ 1**, confirming correct standardization.\n  - The presence of large minimum and maximum values shows that extreme values are **preserved**, not suppressed.\n\n- With **MinMaxScaler**:\n  - All values are mapped strictly into the **[0, 1] range**.\n  - The compression of values indicates high sensitivity to extreme values, as the entire scaling depends on the global minimum and maximum.\n\n- After applying **RobustScaler**:\n  - The median is centered close to **0**, and scaling is based on the **interquartile range (IQR)**.\n  - Compared to the other scalers, the influence of extreme values is reduced, resulting in a more stable spread.\n\n- Overall, the table demonstrates that while all three techniques normalize the feature, they differ significantly in **outlier handling, centering strategy, and scale compression**, which can influence downstream learning behavior.","metadata":{}},{"cell_type":"markdown","source":"## Downstream Consistency Check\n\nA simple regression model is used **only as an evaluation probe**\nto verify whether different scaling strategies distort the learning signal.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ndef evaluate_model(X_train_scaled, X_test_scaled):\n    model = LinearRegression()\n    model.fit(X_train_scaled, y_train)\n    preds = model.predict(X_test_scaled)\n    return {\n        \"MAE\": mean_absolute_error(y_test, preds),\n        \"RMSE\": np.sqrt(mean_squared_error(y_test, preds)),\n        \"R2\": r2_score(y_test, preds)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:15:10.873019Z","iopub.execute_input":"2026-01-19T11:15:10.873405Z","iopub.status.idle":"2026-01-19T11:15:11.007356Z","shell.execute_reply.started":"2026-01-19T11:15:10.873374Z","shell.execute_reply":"2026-01-19T11:15:11.006222Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"X_test_standard = standard_scaler.transform(X_test)\nX_test_minmax = minmax_scaler.transform(X_test)\nX_test_robust = robust_scaler.transform(X_test)\n\nresults = pd.DataFrame({\n    \"StandardScaler\": evaluate_model(X_train_standard, X_test_standard),\n    \"MinMaxScaler\": evaluate_model(X_train_minmax, X_test_minmax),\n    \"RobustScaler\": evaluate_model(X_train_robust, X_test_robust)\n}).T\n\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T11:15:25.031543Z","iopub.execute_input":"2026-01-19T11:15:25.032055Z","iopub.status.idle":"2026-01-19T11:15:25.10213Z","shell.execute_reply.started":"2026-01-19T11:15:25.032022Z","shell.execute_reply":"2026-01-19T11:15:25.101149Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                         MAE          RMSE        R2\nStandardScaler  51657.465162  70517.833856  0.620518\nMinMaxScaler    51657.465162  70517.833856  0.620518\nRobustScaler    51657.465162  70517.833856  0.620518","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAE</th>\n      <th>RMSE</th>\n      <th>R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>StandardScaler</th>\n      <td>51657.465162</td>\n      <td>70517.833856</td>\n      <td>0.620518</td>\n    </tr>\n    <tr>\n      <th>MinMaxScaler</th>\n      <td>51657.465162</td>\n      <td>70517.833856</td>\n      <td>0.620518</td>\n    </tr>\n    <tr>\n      <th>RobustScaler</th>\n      <td>51657.465162</td>\n      <td>70517.833856</td>\n      <td>0.620518</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Observations on Downstream Evaluation\n\n- All three scaling techniques — **StandardScaler**, **MinMaxScaler**, and **RobustScaler** — result in **identical MAE, RMSE, and R² values** on the test set.\n\n- This indicates that, for this dataset, different scaling methods **do not alter the underlying predictive signal**, even though they transform feature distributions differently.\n\n- The result reinforces that scaling primarily affects **data representation, numerical stability, and outlier handling**, rather than guaranteeing improvements in predictive performance.\n\n- The consistency across metrics confirms that the comparison is fair and controlled, as each scaler preserves the same information content while applying different normalization strategies.\n\n- This outcome aligns with the objective of the case study: to compare **how scaling techniques behave**, not to force metric-level improvements.","metadata":{}},{"cell_type":"markdown","source":"## What This Comparison Reveals\n\nThis case study demonstrates that scaling techniques primarily influence\nhow data is **represented and normalized**, rather than directly improving\npredictive performance in all scenarios.\n\nAlthough StandardScaler, MinMaxScaler, and RobustScaler transform feature\ndistributions differently, they preserve the same underlying relationships\nbetween features and the target variable.\n\nAs a result, downstream performance remains consistent, while feature\nrepresentation and sensitivity to outliers vary across scalers.","metadata":{}},{"cell_type":"markdown","source":"## When to Use Which Scaling Technique\n\n| Scenario | Recommended Scaler | Reason |\n|--------|-------------------|--------|\n| Features are normally distributed | StandardScaler | Mean-centered, variance-normalized |\n| Input values must be bounded | MinMaxScaler | Fixed range [0, 1] |\n| Dataset contains outliers | RobustScaler | Uses median and IQR |\n| Distance-based models | StandardScaler / MinMaxScaler | Scale sensitivity |\n| Skewed real-world data | RobustScaler | Stable scaling |\n","metadata":{}},{"cell_type":"markdown","source":"## Limitations\n\n- Only one dataset was analyzed.\n- A single downstream model was used as an evaluation probe.\n- Effects of scaling on regularized or distance-based models were not explored.","metadata":{}},{"cell_type":"markdown","source":"## Final Conclusion\n\nThis case study highlights that choosing a scaling technique is a **data-driven\ndecision**, not a performance shortcut.\n\nStandardScaler, MinMaxScaler, and RobustScaler each serve distinct purposes,\nand understanding their behavior is essential for building reliable and\ninterpretable machine learning pipelines.","metadata":{}}]}